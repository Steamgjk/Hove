Name,Invocations,Avg. Duration(ns),Registers/Thread,Static Shared Memory,Avg. Dynamic Shared Memory,Achieved Occupancy
"void scal_kernel<float, float, int=1, bool=1, int=6, int=5, int=5, int=3>(cublasTransposeParams<float>, float const *, float*, float const *)",720,4318,18,0,0,0.124
"void kernelPointwiseApply1<TensorFillOp<float>, float, unsigned int, int=1>(OffsetInfo<TensorFillOp<float>, float, unsigned int>, float, float)",20,4515,8,0,0,0.23
"_ZN2at6native18elementwise_kernelILi128ELi4EZNS0_16gpu_unary_kernelIZNS0_17gpu_binary_kernelIZNS0_15add_kernel_implIlEEvRNS_14TensorIteratorEN3c106ScalarEEUlllE_EEvS6_RKT_EUllE0_EEvS6_SC_EUliE0_EEviT1_",320,5396,10,0,0,0.059
"void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)",20,6516,32,0,0,0.016
"void AvePoolForward<float, float, bool=1>(int, float const *, int, int, int, int, int, int, int, int, int, int, int, int, float*)",20,10196,30,0,0,0.807
"void at::native::_GLOBAL__N__42_tmpxft_00003bed_00000000_6_SoftMax_cpp1_ii_a3310042::cunn_SoftMaxBackward<int=2, float, float, float, at::native::_GLOBAL__N__42_tmpxft_00003bed_00000000_6_SoftMax_cpp1_ii_a3310042::LogSoftMaxBackwardEpilogue>(float*, float*, float, int)",20,10717,23,0,2048,0.248
"void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)",20,10739,32,256,0,0.016
"void AvePoolBackward<float, float, bool=1>(int, float const *, int, int, int, int, int, int, int, int, int, int, int, int, float*)",20,12004,31,0,0,0.764
"void scalePackedTensor_kernel<float, float>(cudnnTensor4dStruct, float*, float)",300,15705,8,0,0,0.681
"void at::native::_GLOBAL__N__42_tmpxft_00003bed_00000000_6_SoftMax_cpp1_ii_a3310042::cunn_SoftMaxForward<int=2, float, float, float, at::native::_GLOBAL__N__42_tmpxft_00003bed_00000000_6_SoftMax_cpp1_ii_a3310042::LogSoftMaxForwardEpilogue>(float*, float*, int)",20,16643,23,0,2048,0.249
"_ZN2at6native13reduce_kernelILi512ENS0_8ReduceOpIfNS0_14func_wrapper_tIfZNS0_15sum_kernel_implIfffEEvRNS_14TensorIteratorEEUlffE_EEjfEEEEvT0_",80,20342,29,16,0,0.894
"sgemm_sm35_ldg_nt_128x8x128x16x16",5040,33646,127,8212,0,0.134
"void add_tensor_kernel_v3<int=2, float, float, int=128, int=1, int=1, int=4, int=2>(cudnnTensorStruct, float*, cudnnTensorStruct, float const *, float, float, int, int, int)",320,43822,22,0,0,0.806
"sgemm_sm35_ldg_nt_64x16x64x16x16",5040,46947,71,8212,0,0.145
"void calc_bias_diff<int=2, float, float, int=128, int=0>(cudnnTensorStruct, float const *, cudnnTensorStruct, float*, float, float, int)",320,48917,11,512,0,0.67
"_ZN2at6native18elementwise_kernelILi512ELi1EZNS0_17gpu_binary_kernelIZNS0_21threshold_kernel_implIfEEvRNS_14TensorIteratorET_S6_EUlffE_EEvS5_RKS6_EUliE_EEviT1_",640,50664,8,0,0,0.778
"void cudnn::winograd_nonfused::winogradWgradDelta4x4<float, float>(cudnn::winograd_nonfused::WinogradDeltaParams<float, float>)",300,56027,55,16896,0,0.229
"void MaxPoolForward<float, float>(int, float const *, int, int, int, int, int, int, int, int, int, int, int, int, int, int, float*, long*)",100,68834,22,0,0,0.864
"void cudnn::detail::bn_fw_tr_1C11_singleread<float, int=512, bool=1, int=1, int=2, int=0>(cudnnTensorStruct, float const *, cudnn::detail::bn_fw_tr_1C11_singleread<float, int=512, bool=1, int=1, int=2, int=0>, cudnnTensorStruct*, float const *, float const , float, float, float*, float const *, float const *, float const *, float, float, cudnn::reduced_divisor, int, float*, cudnn::detail::bnFwPersistentState*, int, float, float, float, int, float, float, cudnnStatus_t*, bool)",240,72194,35,144,6826,0.69
"void cudnn::winograd_nonfused::winogradWgradData4x4<float, float>(cudnn::winograd_nonfused::WinogradDataParams<float, float>)",300,74442,64,24704,0,0.12
"_ZN2at6native18elementwise_kernelILi512ELi1EZNS0_16gpu_unary_kernelIZNS0_17gpu_binary_kernelIZNS0_15mul_kernel_implIfEEvRNS_14TensorIteratorEEUlffE_EEvS6_RKT_EUlfE0_EEvS6_SA_EUliE_EEviT1_",1440,95003,8,0,0,0.353
"void cudnn::detail::bn_bw_1C11_singleread<float, int=512, bool=1, int=1, int=2, int=0>(float, float, float, float, cudnnTensorStruct, float const *, cudnn::detail::bn_bw_1C11_singleread<float, int=512, bool=1, int=1, int=2, int=0>, float const , cudnn::detail::bn_bw_1C11_singleread<float, int=512, bool=1, int=1, int=2, int=0>, cudnnTensorStruct*, float const *, float*, float const *, float const , float const , float, cudnn::reduced_divisor, int, float*, cudnn::detail::bnBwPersistentState*, int, float, float, float, int, float, cudnnStatus_t*, bool)",240,95882,38,272,13653,0.538
"void sgemm_largek_lds64<bool=0, bool=1, int=5, int=5, int=4, int=4, int=4, int=32>(float*, float const *, float const *, int, int, int, int, int, int, float const *, float const *, float, float, int, int, int*, int*)",720,97625,40,4228,0,0.133
"_ZN2at6native18elementwise_kernelILi512ELi1EZNS0_17gpu_binary_kernelIZNS0_15add_kernel_implIfEEvRNS_14TensorIteratorEN3c106ScalarEEUlffE_EEvS5_RKT_EUliE_EEviT1_",5688,131328,8,0,0,0.359
"void MaxPoolBackward<float, float>(int, float const *, long const *, int, int, int, int, int, int, int, int, int, int, int, int, int, int, float*)",100,138806,32,0,0,0.852
"void im2col4d_kernel<float, int>(im2col4d_params, cudnnConvolutionStruct, cudnnTensor4dStruct, float const *, float*, int)",80,144713,25,0,0,0.131
"void cudnn::winograd_nonfused::winogradWgradOutput4x4<float, float>(cudnn::winograd_nonfused::WinogradWgradOutputParams<float, float>)",300,152265,62,9216,0,0.439
"void cudnn::detail::wgrad_alg0_engine<float, int=512, int=6, int=5, int=3, int=3, int=3, bool=1, int=512>(int, int, int, float const *, int, cudnn::detail::wgrad_alg0_engine<float, int=512, int=6, int=5, int=3, int=3, int=3, bool=1, int=512>*, float const , kernel_grad_params, int, float, int, int, int, int)",20,476703,88,3328,0,0.262
"void cudnn::detail::bn_bw_1C11_kernel_new<float, float, float2, int=512, bool=1, int=1>(float, cudnn::detail::bn_bw_1C11_kernel_new<float, float, float2, int=512, bool=1, int=1>, cudnn::detail::bn_bw_1C11_kernel_new<float, float, float2, int=512, bool=1, int=1>, cudnn::detail::bn_bw_1C11_kernel_new<float, float, float2, int=512, bool=1, int=1>, cudnnTensorStruct, float const *, float, float const , float, cudnnTensorStruct*, cudnn::detail::bn_bw_1C11_kernel_new<float, float, float2, int=512, bool=1, int=1> const *, cudnn::detail::bn_bw_1C11_kernel_new<float, float, float2, int=512, bool=1, int=1>*, cudnn::detail::bn_bw_1C11_kernel_new<flo",80,644033,30,400,0,0.913
"void gemv2N_kernel_val<float, float, float, int=128, int=4, int=4, int=4, int=1>(float, float, cublasGemv2Params_v2<float, float, float>)",80,655591,48,2560,0,0.542
"void gemmk1_kernel<float, int=256, int=5, bool=0, bool=0, bool=1, bool=0>(cublasGemmk1Params<float>, float const *, float const *, float*)",80,667276,17,272,0,0.906
"void gemv2T_kernel_val<float, float, float, int=128, int=16, int=2, int=4, bool=0>(int, int, float, float const *, int, float const *, int, float, float*, int)",80,770278,32,1536,0,0.801
"cudnn_convolve_sgemm_sm35_ldg_nn_32x16x64x8x16",80,781845,83,6420,0,0.233
"void cudnn::detail::bn_fw_tr_1C11_kernel_NCHW<float, float, int=512, bool=1, int=1>(cudnnTensorStruct, float const *, cudnn::detail::bn_fw_tr_1C11_kernel_NCHW<float, float, int=512, bool=1, int=1>, cudnnTensorStruct*, float const *, float const , cudnnTensorStruct*, cudnnTensorStruct*, cudnnTensorStruct**, float const *, float const *, float const *, cudnnTensorStruct*, cudnnTensorStruct*)",80,906514,31,144,0,0.573
"cudnn_dgrad_sm35_ldg_nt_64x16x128x8x32",240,1420912,123,12308,0,0.244
"void cudnn::detail::implicit_convolve_sgemm<float, float, int=1024, int=5, int=5, int=3, int=3, int=3, int=1, bool=1, bool=0, bool=1>(int, int, int, float const *, int, float*, cudnn::detail::implicit_convolve_sgemm<float, float, int=1024, int=5, int=5, int=3, int=3, int=3, int=1, bool=1, bool=0, bool=1>*, kernel_conv_params, int, float, float, int, float, float, int, int)",40,1569250,62,2304,0,0.478
"cudnn_dgrad_sm35_ldg_nt_64x16x64x16x16",20,1577403,79,8212,0,0.367
"void cudnn::detail::implicit_convolve_sgemm<float, float, int=128, int=6, int=7, int=3, int=3, int=5, int=1, bool=1, bool=0, bool=1>(int, int, int, float const *, int, float*, cudnn::detail::implicit_convolve_sgemm<float, float, int=128, int=6, int=7, int=3, int=3, int=5, int=1, bool=1, bool=0, bool=1>*, kernel_conv_params, int, float, float, int, float, float, int, int)",40,1709101,64,6400,0,0.466
"void cudnn::detail::implicit_convolve_sgemm<float, float, int=512, int=6, int=8, int=3, int=3, int=5, int=1, bool=1, bool=0, bool=1>(int, int, int, float const *, int, float*, cudnn::detail::implicit_convolve_sgemm<float, float, int=512, int=6, int=8, int=3, int=3, int=5, int=1, bool=1, bool=0, bool=1>*, kernel_conv_params, int, float, float, int, float, float, int, int)",160,2072364,126,10496,0,0.23
"void cudnn::detail::dgrad_alg1_engine<float, int=128, int=6, int=7, int=3, int=3, int=5, bool=1, bool=0>(int, int, int, float const *, int, float const , int, cudnn::detail::dgrad_alg1_engine<float, int=128, int=6, int=7, int=3, int=3, int=5, bool=1, bool=0>*, kernel_grad_params, int, int, float, int)",20,2347481,64,6400,0,0.46
"void cudnn::detail::dgrad_alg1_engine<float, int=512, int=6, int=5, int=3, int=3, int=3, bool=1, bool=0>(int, int, int, float const *, int, float const , int, cudnn::detail::dgrad_alg1_engine<float, int=512, int=6, int=5, int=3, int=3, int=3, bool=1, bool=0>*, kernel_grad_params, int, int, float, int)",20,3267796,90,3328,0,0.302
